{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib import learn -> deprecated\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data(filename):\n",
    "    with open(filename) as fp:\n",
    "        lines = fp.readlines()\n",
    "        return lines\n",
    "\n",
    "\n",
    "def _length(sentences):\n",
    "    sn_length = [len(sn.split()) for sn in sentences] # get the length of each sentence\n",
    "    return sn_length\n",
    "\n",
    "def _mask(sentences, max_length):\n",
    "    \"\"\"\n",
    "    - Builds a mask array to ignore padded integers for calculating precision, recall and fscore\n",
    "\n",
    "    Args:\n",
    "         sentences: a list of input sentences\n",
    "         max_length: maximum length used for padding sentences\n",
    "\n",
    "    Returns:\n",
    "        mask_array: an array of actual length of sentences\n",
    "\n",
    "    \"\"\"\n",
    "    sn_length = _length(sentences)\n",
    "    mask_array = np.zeros((len(sn_length) * max_length, 1), dtype=np.float64) # initialize mask array by zeros, (len(sn_length) * max_length, 1 means the number of all words in all sentences)\n",
    "    row_num = 0\n",
    "    for length in sn_length:\n",
    "        mask_array[row_num:length+row_num] = 1 # set 1 for actual length of sentences by row_num:length+row_num\n",
    "        row_num += length + (max_length - length) # update row_num by adding length of sentence and padding length\n",
    "    return mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def swbd_data(data_path=None):\n",
    "    train_input_data = _read_data(os.path.join(data_path, \"train_sentences.txt\"))\n",
    "    dev_input_data = _read_data(os.path.join(data_path, \"dev_sentences.txt\"))\n",
    "    test_input_data = _read_data(os.path.join(data_path, \"test_sentences.txt\"))\n",
    "\n",
    "    max_length = max(_length(train_input_data))\n",
    "\n",
    "    # Instantiate the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Fit the tokenizer on your text\n",
    "    tokenizer.fit_on_texts(train_input_data)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    train_input_ids = np.array(tokenizer.texts_to_sequences(train_input_data))\n",
    "    dev_input_ids = np.array(tokenizer.texts_to_sequences(dev_input_data))\n",
    "    test_input_ids = np.array(tokenizer.texts_to_sequences(test_input_data))\n",
    "\n",
    "    train_output_data = _read_data(os.path.join(data_path, \"train_word_labels.txt\"))\n",
    "    dev_output_data = _read_data(os.path.join(data_path, \"dev_word_labels.txt\"))\n",
    "    test_output_data = _read_data(os.path.join(data_path, \"test_word_labels.txt\"))\n",
    "\n",
    "    # For output_vocab_processor, since it's a simple mapping, we can replace it with a manual transformation\n",
    "    label_vocab = {'F': 0, 'E': 1}\n",
    "    train_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in train_output_data])\n",
    "    dev_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in dev_output_data])\n",
    "    test_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in test_output_data])\n",
    "\n",
    "    train_mask = _mask(train_input_data, max_length)\n",
    "    dev_mask = _mask(dev_input_data, max_length)\n",
    "    test_mask = _mask(test_input_data, max_length)\n",
    "\n",
    "    return train_input_ids, \\\n",
    "        dev_input_ids, \\\n",
    "        test_input_ids, \\\n",
    "        train_output_ids, \\\n",
    "        dev_output_ids, \\\n",
    "        test_output_ids, \\\n",
    "        train_mask, \\\n",
    "        dev_mask, \\\n",
    "        test_mask, \\\n",
    "        max_length, \\\n",
    "        tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate on the input and output files, and create minibatches of input, output and mask.\n",
    "def swbd_minibatches(input_ids, output_ids, mask_data, batch_size, num_epochs, max_length, shuffle=True):\n",
    "    \"\"\"\n",
    "    - Iterates on the Switchboard input and output files\n",
    "\n",
    "    Args:\n",
    "        input_ids: one of the input id files from swbd_data\n",
    "        output_ids: one of the output id files from swbd_data\n",
    "        mask_data: one of the mask files from swbd_data\n",
    "        batch_size: int, the batch size\n",
    "        num_epochs: int, the number of training epochs\n",
    "        max_length: int, the maximum length used for padding\n",
    "        shuffle: Boolean, whether to shuffle training data or not\n",
    "\n",
    "    Returns:\n",
    "        tuple (x, y, z): which are minibathes of (input, output, mask)\n",
    "    \"\"\"\n",
    "\n",
    "    output_ids = np.reshape(np.array(output_ids), (-1, max_length))\n",
    "    mask_data = np.reshape(np.array(mask_data), (-1, max_length))\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(input_ids)))\n",
    "        input_ids = input_ids[shuffle_indices]\n",
    "        output_ids = output_ids[shuffle_indices]\n",
    "        mask_data = mask_data[shuffle_indices]\n",
    "\n",
    "    input_ids = np.array([np.concatenate(input_ids, 0)]).T\n",
    "    output_ids = np.array([np.concatenate(output_ids, 0)]).T\n",
    "    mask_data = mask_data.reshape(-1, 1)\n",
    "\n",
    "    data_size = len(input_ids) // max_length\n",
    "    num_batches_per_epoch = data_size // batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = (batch_num * batch_size) * max_length\n",
    "            end_index = (min((batch_num + 1) * batch_size, data_size)) * max_length\n",
    "            x = np.reshape(input_ids[start_index:end_index], (batch_size, max_length))\n",
    "            y = output_ids[start_index:end_index]\n",
    "            z = mask_data[start_index:end_index]\n",
    "            yield (x, y, z)\n",
    "            \n",
    "            \n",
    "def batch_iter(input_id, max_length, mask):\n",
    "    \"\"\"\n",
    "    - Iterates on input data (usef for prediction)\n",
    "\n",
    "    Args:\n",
    "        input_id: list of input sentences mapped to integers\n",
    "        max_length: maximum length of sentences\n",
    "        mask: list of actual length of sentences\n",
    "     \n",
    "    Returns:\n",
    "        tuple (x_input, z_mask): which are minibathes of (input, mask)\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.array(input_id)\n",
    "    for sn in range(len(input_id)):\n",
    "        start = sn * max_length\n",
    "        end = (1 + sn) * max_length\n",
    "        x_input = x[sn : sn + 1]\n",
    "        z_mask = mask[start:end]\n",
    "        yield (x_input, z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- End of Reader ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "class ACNN(object):\n",
    "    \"\"\"\n",
    "\n",
    "    An ACNN for disfluency detection:\n",
    "    which includes an embedding layer, followed by a drop-out layer, \n",
    "    an auto-correlational layer, two convolutional layers and a sigmoid layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length, num_classes, vocab_size, embedding_size, \n",
    "                 conv1_filter_sizes, conv2_filter_sizes, conv3_filter_sizes, \n",
    "                 num_filters, embed_initial, weight_initial, device_name):\n",
    "        \n",
    "        # Inputs\n",
    "        self.input_x = Input(shape=(max_length,), dtype='int32', name=\"input_x\")\n",
    "        self.input_y = Input(shape=(num_classes,), dtype='float32', name=\"input_y\")\n",
    "        self.input_z = Input(shape=(1,), dtype='float32', name=\"input_z\")\n",
    "        self.dropout_keep_prob = tf.Variable(initial_value=0.5, trainable=False, name=\"dropout_keep_prob\")\n",
    "        self.l2_reg_lambda = tf.Variable(initial_value=0.0, trainable=False, name=\"l2_reg_lambda\")\n",
    "        self.batch_size = tf.Variable(initial_value=32, trainable=False, shape=[], name=\"batch_size\")\n",
    "\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Embedding Layer\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.embedding = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embedding_size, \n",
    "                                   embeddings_initializer='uniform',\n",
    "                                   input_length=max_length,\n",
    "                                   name=\"embedding\")\n",
    "        self.embedded_words = self.embedding(self.input_x)\n",
    "        self.embedded_words_expanded = tf.expand_dims(self.embedded_words, -1)\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Drop-out Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        self.dropout = Dropout(rate=1-self.dropout_keep_prob.numpy())\n",
    "        self.h_drop = self.dropout(self.embedded_words_expanded)\n",
    "        \n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Auto-Correlation Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        \"\"\"\n",
    "        The auto-correlation layer includes two parts:\n",
    "            1. An auto-correlated tensor which is constructed by comparing each input vector u \n",
    "            to the input vector v using a binary function f. \n",
    "            The auto-correlated tensor is then convolved with 3D or 4D kernels B of different sizes.\n",
    "\n",
    "            2. A vanilla CNN layer which convolves the input tensor with kernels A of different sizes.\n",
    "            \n",
    "            Each kernel group A and B outputs a tensor of the same size which are added element-wise \n",
    "            to produce the feature representation that is passed to further convolutional layers. \n",
    "            For more details, read https://www.aclweb.org/anthology/D18-1490.pdf.\n",
    "        \"\"\"\n",
    "        self.combine_acnn_features = []\n",
    "        for i, filter_size in enumerate(conv1_filter_sizes):\n",
    "            # First part of the ACNN layer:\n",
    "            kernel_B_shape = [conv1_filter_sizes[i], conv1_filter_sizes[i], num_filters] \n",
    "            kernel_B = tf.Variable(tf.random.truncated_normal(kernel_B_shape, stddev=weight_initial), name=\"w\")\n",
    "            flat_kernel_B = tf.reshape(kernel_B, (-1, num_filters))\n",
    "            patches = tf.image.extract_patches(self.h_drop,\n",
    "                                               sizes=[1, conv1_filter_sizes[i], embedding_size, 1],\n",
    "                                               strides=[1, 1, embedding_size, 1],\n",
    "                                               rates=[1, 1, 1, 1],\n",
    "                                               padding=\"SAME\",\n",
    "                                               name=\"patches\"\n",
    "                                               )\n",
    "            reshaped_patches = tf.reshape(patches, [-1, conv1_filter_sizes[i], embedding_size])\n",
    "            function_f = tf.einsum('ijl,ikl->ijk', reshaped_patches, reshaped_patches) \n",
    "            reshaped_function_f = tf.reshape(function_f, [self.batch_size.numpy() * max_length, -1])\n",
    "            auto_correlated_input = tf.reshape(tf.matmul(\n",
    "                                               reshaped_function_f, flat_kernel_B, name='auto_cor'\n",
    "                                               ), (self.batch_size.numpy(), max_length, 1, -1))\n",
    "\n",
    "            # Second part of the ACNN layer:\n",
    "            conv1 = Conv2D(filters=num_filters, \n",
    "                           kernel_size=(conv1_filter_sizes[i], embedding_size), \n",
    "                           strides=(1, embedding_size), \n",
    "                           padding='same', \n",
    "                           activation='relu', \n",
    "                           name=\"conv1\")\n",
    "            convolved_input = conv1(self.h_drop)\n",
    "\n",
    "            # Element-wise addition of the outputs of 1st and 2nd parts of the ACNN layer:\n",
    "            added_outputs = tf.add(auto_correlated_input, convolved_input)\n",
    "            auto_correlation = tf.nn.relu(added_outputs)\n",
    "            self.combine_acnn_features.append(auto_correlation)\n",
    "\n",
    "        # Combine all the ACNN features:\n",
    "        num_filters_total = num_filters * len(conv1_filter_sizes)\n",
    "        self.all_acnn_features = tf.concat(self.combine_acnn_features, 3)\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Convolutional Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.combine_conv3_features = []\n",
    "        conv3_input = tf.reshape(self.all_acnn_features, [-1, max_length, num_filters_total, 1])\n",
    "        for k, filter_size in enumerate(conv3_filter_sizes):\n",
    "            conv3 = Conv2D(filters=num_filters, \n",
    "                           kernel_size=(conv3_filter_sizes[k], num_filters_total), \n",
    "                           strides=(1, num_filters_total), \n",
    "                           padding='same', \n",
    "                           activation='relu', \n",
    "                           name=\"conv3\")\n",
    "            conv3_output = conv3(conv3_input)\n",
    "            self.combine_conv3_features.append(conv3_output)\n",
    "\n",
    "        # Combine all the conv3 features:\n",
    "        num_filters_total = num_filters * len(conv3_filter_sizes)\n",
    "        self.all_conv3_features = tf.concat(self.combine_conv3_features, 3)\n",
    "        self.reshaped_conv3_features = tf.reshape(self.all_conv3_features, [-1, num_filters_total])\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                         1-Width Convolution\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Fully Connected Layer\n",
    "        self.l2_loss = tf.constant(0.0) # keeping track of l2 regularization loss\n",
    "        self.local4 = Dense(units=128, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            activation='relu', \n",
    "                            name=\"local4\")\n",
    "        local4_output = self.local4(self.reshaped_conv3_features)\n",
    "        self.l2_loss += tf.nn.l2_loss(self.local4.weights[0]) # weights[0] should give the kernel weights\n",
    "\n",
    "        # Final scores and predictions\n",
    "        self.output = Dense(units=num_classes, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            activation='sigmoid', \n",
    "                            name=\"output\")\n",
    "        self.scores = self.output(local4_output)\n",
    "        self.l2_loss += tf.nn.l2_loss(self.output.weights[0]) # weights[0] should give the kernel weights\n",
    "        self.probs = tf.sigmoid(self.scores, name=\"sigmoid\") \n",
    "        condition = tf.less(self.probs, tf.fill(tf.shape(self.probs), 0.5))\n",
    "        self.predictions = tf.where(\n",
    "            condition, tf.zeros(tf.shape(self.probs)), tf.ones(tf.shape(self.probs))\n",
    "            ) # if prob >= 0.5 ==> 1 (i.e. disfluent); else ==> 0 (i.e. fluent)\n",
    "\n",
    "\n",
    "        # Calculate sigmoid cross entropy loss\n",
    "        self.loss_fn = BinaryCrossentropy(from_logits=True)\n",
    "        losses = self.loss_fn(self.input_y, self.scores)\n",
    "        masked_losses = tf.transpose(self.input_z) * losses\n",
    "        self.loss = (tf.reduce_sum(masked_losses) / tf.cast(self.batch_size, \"float32\")) \\\n",
    "                    + (self.l2_reg_lambda * self.l2_loss)\n",
    "\n",
    "        # Calculate f-score\n",
    "        fscore_mask = tf.cast(self.input_z, \"int64\")\n",
    "        predictions = tf.cast(self.predictions, \"int64\")\n",
    "        input_y = tf.cast(self.input_y, \"int64\")\n",
    "\n",
    "        # e ==> num of edited predictions\n",
    "        masked_prediction = fscore_mask * predictions\n",
    "        e = tf.reduce_sum(masked_prediction)\n",
    "        self.nprediction = tf.cast(e, dtype=tf.int32, name=\"nprediction\")\n",
    "\n",
    "        # g ==> num of edited words in gold set\n",
    "        masked_input_y = fscore_mask * input_y\n",
    "        g = tf.reduce_sum(masked_input_y)\n",
    "\n",
    "## End of ACNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 290  # Dimensionality of word embedding\n",
    "num_filters = 120  # Number of filters per filter size\n",
    "conv1_filter_sizes = \"12,7\"  # Comma-separated conv1 filter sizes\n",
    "conv2_filter_sizes = \"10,6\"  # Comma-separated conv2 filter sizes\n",
    "conv3_filter_sizes = \"8,5\"  # Comma-separated conv3 filter sizes\n",
    "dropout_keep_prob = 0.53  # Dropout keep probability\n",
    "l2_reg_lambda = 0.23  # L2 regularization lambda\n",
    "learning_rate = 0.001  # Learning rate\n",
    "embed_initial = 0.09  # The standard deviation of word embedding initializer\n",
    "weight_initial = 0.09  # The standard deviation of weight initializers\n",
    "batch_size = 25  # Training batch size\n",
    "num_epochs = 25  # Number of training epochs\n",
    "num_checkpoints = 5  # Number of checkpoints to store\n",
    "dev_batch_size = 143  # Dev batch size\n",
    "\n",
    "# other parameters\n",
    "device_name = '/cpu:0'  # Device name to be used in ACNN layer\n",
    "allow_soft_placement = True  # Allow device soft device placement\n",
    "log_device_placement = False  # Log placement of ops on devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#                        Data Preparation\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Loading training, dev and test data:\n",
    "print(\"Loading data...\")\n",
    "\n",
    "x_train, x_dev, x_test, y_train, y_dev, y_test, z_train, z_dev, z_test, max_length, vocab = swbd_data(data_path=\"data\") # need to change data_path to your own path\n",
    "\n",
    "# Evaluate model on dev data and save the model at the end of each epoch:\n",
    "\n",
    "evaluate_every = (len(x_train) - 1) // batch_size\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#                           Training\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "graph = tf.Graph() # create empty graph for training\n",
    "with graph.as_default():\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = ACNN(\n",
    "            max_length=max_length,\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocab.word_index) + 1,\n",
    "            embedding_size=embed_dim,\n",
    "            conv1_filter_sizes=list(map(int, conv1_filter_sizes.split(\",\"))),\n",
    "            conv2_filter_sizes=list(map(int, conv2_filter_sizes.split(\",\"))),\n",
    "            conv3_filter_sizes=list(map(int, conv3_filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            embed_initial=embed_initial,\n",
    "            weight_initial=weight_initial,\n",
    "            device_name=device_name\n",
    "        )\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = cnn.loss\n",
    "        grads = tape.gradient(loss, cnn.trainable_variables)\n",
    "        grads_and_vars = zip(grads, cnn.trainable_variables)\n",
    "\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional):\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output dir for models and summaries:\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(FLAGS.checkpoint_dir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Summaries for loss:\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "\n",
    "        # Train Summaries:\n",
    "        train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.create_file_writer(train_summary_dir)\n",
    "\n",
    "        # Dev summaries:\n",
    "        dev_summary_op = tf.summary.merge([loss_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.create_file_writer(dev_summary_dir)\n",
    "\n",
    "        '''\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        '''\n",
    "\n",
    "        # Write vocabulary:\n",
    "        vocab.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        print (\"+\"*50)\n",
    "        print (\"EPOCH 1\")          \n",
    "        print (\"+\"*50)\n",
    "        cost = [] # saving loss of each training step\n",
    "        e = [] # saving num of edited predictions in training set\n",
    "        c = [] # saving num of correct edited predictions in training set\n",
    "        g = [] # saving num of ground truth edited labels in training set\n",
    "\n",
    "\n",
    "        def train_step(x_batch, y_batch,z_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            cnn.input_x = x_batch\n",
    "            cnn.input_y = y_batch\n",
    "            cnn.input_z = z_batch\n",
    "            cnn.dropout_keep_prob = dropout_keep_prob\n",
    "            cnn.l2_reg_lambda = l2_reg_lambda\n",
    "            cnn.batch_size = batch_size\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = cnn.loss\n",
    "            grads = tape.gradient(loss, cnn.trainable_variables)\n",
    "            grads_and_vars = zip(grads, cnn.trainable_variables)\n",
    "\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "            step, summaries, nprediction, ncorrect, ntarget = train_op, global_step, train_summary_op, cnn.loss, cnn.nprediction, cnn.ncorrect, cnn.ntarget\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=step)\n",
    "\n",
    "            cost.append(loss)\n",
    "            e.append(nprediction)\n",
    "            c.append(ncorrect)\n",
    "            g.append(ntarget)\n",
    "\n",
    "        cost_dev = [] # saving loss of each dev step\n",
    "        e_dev = [] # saving num of edited predictions in dev set\n",
    "        c_dev = [] # saving num of correct edited predictions in dev set\n",
    "        g_dev = [] # saving num of ground truth edited labels in dev set\n",
    "\n",
    "        def dev_step(x_batch, y_batch, z_dev, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            cnn.input_x = x_batch\n",
    "            cnn.input_y = y_batch\n",
    "            cnn.input_z = z_dev\n",
    "            cnn.dropout_keep_prob = 1.0\n",
    "            cnn.l2_reg_lambda = 0.0\n",
    "            cnn.batch_size = dev_batch_size\n",
    "\n",
    "            step, summaries, loss, dev_nprediction, dev_ncorrect, dev_ntarget = global_step, dev_summary_op, cnn.loss, cnn.nprediction, cnn.ncorrect, cnn.ntarget\n",
    "\n",
    "            cost_dev.append(loss)\n",
    "            e_dev.append(dev_nprediction)\n",
    "            c_dev.append(dev_ncorrect)\n",
    "            g_dev.append(dev_ntarget)\n",
    "            if writer:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('loss', loss, step=step)\n",
    "\n",
    "        cost_test = [] # saving loss of each test step\n",
    "        e_test = [] # saving num of edited predictions in test set\n",
    "        c_test = [] # saving num of correct edited predictions in test set\n",
    "        g_test = [] # saving num of ground truth edited labels in test set\n",
    "\n",
    "\n",
    "        def test_step(x_batch, y_batch, z_dev):\n",
    "            \"\"\"\n",
    "            Evaluates model on a test set\n",
    "            \"\"\"\n",
    "            cnn.input_x = x_batch\n",
    "            cnn.input_y = y_batch\n",
    "            cnn.input_z = z_dev\n",
    "            cnn.dropout_keep_prob = 1.0\n",
    "            cnn.l2_reg_lambda = 0.0\n",
    "            cnn.batch_size = len(x_test)\n",
    "\n",
    "            loss, test_nprediction, test_ncorrect, test_ntarget = cnn.loss, cnn.nprediction, cnn.ncorrect, cnn.ntarget\n",
    "\n",
    "            cost_test.append(loss)\n",
    "            e_test.append(test_nprediction)\n",
    "            c_test.append(test_ncorrect)\n",
    "            g_test.append(test_ntarget)\n",
    "        \n",
    "        # Generate batches\n",
    "\n",
    "        batches = swbd_minibatches(x_train, y_train, z_train, batch_size, num_epochs, max_length, shuffle=True)\n",
    "\n",
    "        # Training loop:\n",
    "        epoch_counter = 1\n",
    "        current_step = 0\n",
    "        # checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=cnn)\n",
    "        # manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        for batch in batches:\n",
    "            x_batch = batch[0]\n",
    "            y_batch = batch[1]\n",
    "            z_batch = batch[2]\n",
    "\n",
    "            train_step(x_batch, y_batch, z_batch)\n",
    "            current_step += 1\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print (\"EPOCH {}: train loss={:.3}, precision={:.3}, recall={:.3}, fscore={:.3}\".format(\n",
    "                    epoch_counter, np.mean(cost), sum(c) / (sum(e) + (1e-100)), \\\n",
    "                    sum(c) / (sum(g) + (1e-100)), (2 * sum(c)) / (sum(g) + sum(e) + 1e-100)\n",
    "                ))\n",
    "                print (\"len {}\".format(len(cost)))\n",
    "                cost = []\n",
    "                e = []\n",
    "                c = []\n",
    "                g = []\n",
    "\n",
    "                # Evaluating the model on the dev set at the end of each training epoch:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_batches = swbd_minibatches(\n",
    "                    x_dev, y_dev, z_dev, dev_batch_size, \\\n",
    "                    num_epochs=1, max_length=max_length, shuffle=False\n",
    "                    )\n",
    "\n",
    "                for batch_num in dev_batches:\n",
    "                    x_dev_batch = batch_num[0]\n",
    "                    y_dev_batch = batch_num[1]\n",
    "                    z_dev_batch = batch_num[2]\n",
    "\n",
    "                    dev_step(x_dev_batch, y_dev_batch, z_dev_batch, writer=dev_summary_writer)\n",
    "\n",
    "                print (\"\\nEPOCH {}: eval loss={:.3}, precision={:.3}, recall={:.3}, fscore={:.3}\".format(\n",
    "                    epoch_counter, np.mean(cost_dev), sum(c_dev) / (sum(e_dev) + (1e-100)), \\\n",
    "                    sum(c_dev) / (sum(g_dev) + (1e-100)), (2 * sum(c_dev)) / (sum(g_dev) + sum(e_dev) + 1e-100)\n",
    "                ))\n",
    "\n",
    "                cost_dev = []\n",
    "                e_dev = []\n",
    "                c_dev = []\n",
    "                g_dev = []\n",
    "                epoch_counter += 1\n",
    "\n",
    "                print (\"+\"*50)\n",
    "                print (\"EPOCH {}\".format(epoch_counter))          \n",
    "                print (\"+\"*50)\n",
    "            '''\n",
    "            if current_step % evaluate_every == 0:\n",
    "                path = manager.save(checkpoint_number=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            ''' \n",
    "\n",
    "        # Testing the model on the test set:\n",
    "        print(\"\\nTest:\")\n",
    "        test_batches = swbd_minibatches(\n",
    "            x_test, y_test, z_test, dev_batch_size, \\\n",
    "            num_epochs=1, max_length=max_length, shuffle=False\n",
    "            )\n",
    "        \n",
    "        for batch_num in test_batches:\n",
    "            x_test_batch = batch_num[0]\n",
    "            y_test_batch = batch_num[1]\n",
    "            z_test_batch = batch_num[2]\n",
    "\n",
    "            test_step(x_test_batch, y_test_batch, z_test_batch)\n",
    "\n",
    "        print (\"\\nTest loss={:.3}, precision={:.3}, recall={:.3}, fscore={:.3}\".format(\n",
    "            np.mean(cost_test), sum(c_test) / (sum(e_test) + (1e-100)), \\\n",
    "            sum(c_test) / (sum(g_test) + (1e-100)), (2 * sum(c_test)) / (sum(g_test) + sum(e_test) + 1e-100)\n",
    "        ))\n",
    "        print (\"len {}\".format(len(cost_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"\" # Data source for input file\n",
    "checkpoint_dir = \"\" # Checkpoint directory from training run\n",
    "allow_soft_placement = True  # Allow device soft device placement\n",
    "log_device_placement = False  # Log placement of ops on devices\n",
    "output_path = \"\" # Directory to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = _read_data(input_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Read your data\n",
    "input_data = _read_data(input_path)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on your text\n",
    "tokenizer.fit_on_texts(input_data)\n",
    "\n",
    "# Transform your text into sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_ids = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Save tokenizer\n",
    "with open(os.path.join(checkpoint_dir, \"..\", \"vocab\"), 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Create result file:\n",
    "output_file = open(os.path.join(FLAGS.output_path, \"results.txt\"),'w')\n",
    "\n",
    "# Evaluation:\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "checkpoint_file = tf.train.latest_checkpoint(os.path.join(\n",
    "    checkpoint_dir, \"checkpoints\")\n",
    "    )\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "''' \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables:\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name:\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "       \n",
    "        # Tensors we want to evaluate:\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches:\n",
    "        sn_length = reader._length(input_data)\n",
    "        max_length = max(sn_length)\n",
    "        mask = reader._mask(input_data, max_length)\n",
    "        batches = reader.batch_iter(input_id, max_length, mask)\n",
    "        batch_size = graph.get_operation_by_name(\"batch_size\").outputs[0]\n",
    "            \n",
    "        # Collect the predictions:  \n",
    "        indx = 0\n",
    "        for batch in batches:\n",
    "            x_batch = batch[0]\n",
    "            z_batch = batch[1]\n",
    "            batch_predictions = sess.run(\n",
    "                predictions, {input_x: x_batch, batch_size: 1, dropout_keep_prob: 1.0}\n",
    "                )    \n",
    "            words = input_data[indx].split()\n",
    "            # \"E\" stands for disfluent words and \"F\" for fluent words:\n",
    "            for i in range(sn_length[indx]):\n",
    "                label = 'E' if batch_predictions[i] == 1 else 'F'\n",
    "                output_file.write(words[i]+' '+label+' ')\n",
    "            output_file.write('\\n')\n",
    "            indx += 1\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
