{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib import learn -> deprecated\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data(filename):\n",
    "    with open(filename) as fp:\n",
    "        lines = fp.readlines()\n",
    "        return lines\n",
    "\n",
    "\n",
    "def _length(sentences):\n",
    "    sn_length = [len(sn.split()) for sn in sentences] # get the length of each sentence\n",
    "    return sn_length\n",
    "\n",
    "def _mask(sentences, max_length):\n",
    "    \"\"\"\n",
    "    - Builds a mask array to ignore padded integers for calculating precision, recall and fscore\n",
    "\n",
    "    Args:\n",
    "         sentences: a list of input sentences\n",
    "         max_length: maximum length used for padding sentences\n",
    "\n",
    "    Returns:\n",
    "        mask_array: an array of actual length of sentences\n",
    "\n",
    "    \"\"\"\n",
    "    sn_length = _length(sentences)\n",
    "    mask_array = np.zeros((len(sn_length) * max_length, 1), dtype=np.float64) # initialize mask array by zeros, (len(sn_length) * max_length, 1 means the number of all words in all sentences)\n",
    "    row_num = 0\n",
    "    for length in sn_length:\n",
    "        mask_array[row_num:length+row_num] = 1 # set 1 for actual length of sentences by row_num:length+row_num\n",
    "        row_num += length + (max_length - length) # update row_num by adding length of sentence and padding length\n",
    "    return mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def swbd_data(data_path=None):\n",
    "    train_input_data = _read_data(os.path.join(data_path, \"swbd.train.txt\"))\n",
    "    dev_input_data = _read_data(os.path.join(data_path, \"swbd.dev.txt\"))\n",
    "    test_input_data = _read_data(os.path.join(data_path, \"swbd.test.txt\"))\n",
    "\n",
    "    max_length = max(_length(train_input_data))\n",
    "\n",
    "    # Instantiate the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Fit the tokenizer on your text\n",
    "    tokenizer.fit_on_texts(train_input_data)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    train_input_ids = np.array(tokenizer.texts_to_sequences(train_input_data))\n",
    "    dev_input_ids = np.array(tokenizer.texts_to_sequences(dev_input_data))\n",
    "    test_input_ids = np.array(tokenizer.texts_to_sequences(test_input_data))\n",
    "\n",
    "    train_output_data = _read_data(os.path.join(data_path, \"swbd.train.label.txt\"))\n",
    "    dev_output_data = _read_data(os.path.join(data_path, \"swbd.dev.label.txt\"))\n",
    "    test_output_data = _read_data(os.path.join(data_path, \"swbd.test.label.txt\"))\n",
    "\n",
    "    # For output_vocab_processor, since it's a simple mapping, we can replace it with a manual transformation\n",
    "    label_vocab = {'F': 0, 'E': 1}\n",
    "    train_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in train_output_data])\n",
    "    dev_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in dev_output_data])\n",
    "    test_output_ids = np.array([[label_vocab[word] for word in sentence.split()] for sentence in test_output_data])\n",
    "\n",
    "    train_mask = _mask(train_input_data, max_length)\n",
    "    dev_mask = _mask(dev_input_data, max_length)\n",
    "    test_mask = _mask(test_input_data, max_length)\n",
    "\n",
    "    return train_input_ids, \\\n",
    "        dev_input_ids, \\\n",
    "        test_input_ids, \\\n",
    "        train_output_ids, \\\n",
    "        dev_output_ids, \\\n",
    "        test_output_ids, \\\n",
    "        train_mask, \\\n",
    "        dev_mask, \\\n",
    "        test_mask, \\\n",
    "        max_length, \\\n",
    "        tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate on the input and output files, and create minibatches of input, output and mask.\n",
    "def swbd_minibatches(input_ids, output_ids, mask_data, batch_size, num_epochs, max_length, shuffle=True):\n",
    "    \"\"\"\n",
    "    - Iterates on the Switchboard input and output files\n",
    "\n",
    "    Args:\n",
    "        input_ids: one of the input id files from swbd_data\n",
    "        output_ids: one of the output id files from swbd_data\n",
    "        mask_data: one of the mask files from swbd_data\n",
    "        batch_size: int, the batch size\n",
    "        num_epochs: int, the number of training epochs\n",
    "        max_length: int, the maximum length used for padding\n",
    "        shuffle: Boolean, whether to shuffle training data or not\n",
    "\n",
    "    Returns:\n",
    "        tuple (x, y, z): which are minibathes of (input, output, mask)\n",
    "    \"\"\"\n",
    "\n",
    "    output_ids = np.reshape(np.array(output_ids), (-1, max_length))\n",
    "    mask_data = np.reshape(np.array(mask_data), (-1, max_length))\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(input_ids)))\n",
    "        input_ids = input_ids[shuffle_indices]\n",
    "        output_ids = output_ids[shuffle_indices]\n",
    "        mask_data = mask_data[shuffle_indices]\n",
    "\n",
    "    input_ids = np.array([np.concatenate(input_ids, 0)]).T\n",
    "    output_ids = np.array([np.concatenate(output_ids, 0)]).T\n",
    "    mask_data = mask_data.reshape(-1, 1)\n",
    "\n",
    "    data_size = len(input_ids) // max_length\n",
    "    num_batches_per_epoch = data_size // batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = (batch_num * batch_size) * max_length\n",
    "            end_index = (min((batch_num + 1) * batch_size, data_size)) * max_length\n",
    "            x = np.reshape(input_ids[start_index:end_index], (batch_size, max_length))\n",
    "            y = output_ids[start_index:end_index]\n",
    "            z = mask_data[start_index:end_index]\n",
    "            yield (x, y, z)\n",
    "            \n",
    "            \n",
    "def batch_iter(input_id, max_length, mask):\n",
    "    \"\"\"\n",
    "    - Iterates on input data (usef for prediction)\n",
    "\n",
    "    Args:\n",
    "        input_id: list of input sentences mapped to integers\n",
    "        max_length: maximum length of sentences\n",
    "        mask: list of actual length of sentences\n",
    "     \n",
    "    Returns:\n",
    "        tuple (x_input, z_mask): which are minibathes of (input, mask)\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.array(input_id)\n",
    "    for sn in range(len(input_id)):\n",
    "        start = sn * max_length\n",
    "        end = (1 + sn) * max_length\n",
    "        x_input = x[sn : sn + 1]\n",
    "        z_mask = mask[start:end]\n",
    "        yield (x_input, z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- End of Reader ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "class ACNN(object):\n",
    "    \"\"\"\n",
    "\n",
    "    An ACNN for disfluency detection:\n",
    "    which includes an embedding layer, followed by a drop-out layer, \n",
    "    an auto-correlational layer, two convolutional layers and a sigmoid layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length, num_classes, vocab_size, embedding_size, \n",
    "                 conv1_filter_sizes, conv2_filter_sizes, conv3_filter_sizes, \n",
    "                 num_filters, embed_initial, weight_initial, device_name):\n",
    "        \n",
    "        # Inputs\n",
    "        self.input_x = Input(shape=(max_length,), dtype='int32', name=\"input_x\")\n",
    "        self.input_y = Input(shape=(num_classes,), dtype='float32', name=\"input_y\")\n",
    "        self.input_z = Input(shape=(1,), dtype='float32', name=\"input_z\")\n",
    "        self.dropout_keep_prob = tf.Variable(initial_value=0.5, trainable=False, name=\"dropout_keep_prob\")\n",
    "        self.l2_reg_lambda = tf.Variable(initial_value=0.0, trainable=False, name=\"l2_reg_lambda\")\n",
    "        self.batch_size = tf.Variable(initial_value=32, trainable=False, shape=[], name=\"batch_size\")\n",
    "\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Embedding Layer\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.embedding = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embedding_size, \n",
    "                                   embeddings_initializer='uniform',\n",
    "                                   input_length=max_length,\n",
    "                                   name=\"embedding\")\n",
    "        self.embedded_words = self.embedding(self.input_x)\n",
    "        self.embedded_words_expanded = tf.expand_dims(self.embedded_words, -1)\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Drop-out Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        self.dropout = Dropout(rate=1-self.dropout_keep_prob.numpy())\n",
    "        self.h_drop = self.dropout(self.embedded_words_expanded)\n",
    "        \n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Auto-Correlation Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        \"\"\"\n",
    "        The auto-correlation layer includes two parts:\n",
    "            1. An auto-correlated tensor which is constructed by comparing each input vector u \n",
    "            to the input vector v using a binary function f. \n",
    "            The auto-correlated tensor is then convolved with 3D or 4D kernels B of different sizes.\n",
    "\n",
    "            2. A vanilla CNN layer which convolves the input tensor with kernels A of different sizes.\n",
    "            \n",
    "            Each kernel group A and B outputs a tensor of the same size which are added element-wise \n",
    "            to produce the feature representation that is passed to further convolutional layers. \n",
    "            For more details, read https://www.aclweb.org/anthology/D18-1490.pdf.\n",
    "        \"\"\"\n",
    "        self.combine_acnn_features = []\n",
    "        for i, filter_size in enumerate(conv1_filter_sizes):\n",
    "            # First part of the ACNN layer:\n",
    "            kernel_B_shape = [conv1_filter_sizes[i], conv1_filter_sizes[i], num_filters] \n",
    "            kernel_B = tf.Variable(tf.random.truncated_normal(kernel_B_shape, stddev=weight_initial), name=\"w\")\n",
    "            flat_kernel_B = tf.reshape(kernel_B, (-1, num_filters))\n",
    "            patches = tf.image.extract_patches(self.h_drop,\n",
    "                                               sizes=[1, conv1_filter_sizes[i], embedding_size, 1],\n",
    "                                               strides=[1, 1, embedding_size, 1],\n",
    "                                               rates=[1, 1, 1, 1],\n",
    "                                               padding=\"SAME\",\n",
    "                                               name=\"patches\"\n",
    "                                               )\n",
    "            reshaped_patches = tf.reshape(patches, [-1, conv1_filter_sizes[i], embedding_size])\n",
    "            function_f = tf.einsum('ijl,ikl->ijk', reshaped_patches, reshaped_patches) \n",
    "            reshaped_function_f = tf.reshape(function_f, [self.batch_size.numpy() * max_length, -1])\n",
    "            auto_correlated_input = tf.reshape(tf.matmul(\n",
    "                                               reshaped_function_f, flat_kernel_B, name='auto_cor'\n",
    "                                               ), (self.batch_size.numpy(), max_length, 1, -1))\n",
    "\n",
    "            # Second part of the ACNN layer:\n",
    "            conv1 = Conv2D(filters=num_filters, \n",
    "                           kernel_size=(conv1_filter_sizes[i], embedding_size), \n",
    "                           strides=(1, embedding_size), \n",
    "                           padding='same', \n",
    "                           activation='relu', \n",
    "                           name=\"conv1\")\n",
    "            convolved_input = conv1(self.h_drop)\n",
    "\n",
    "            # Element-wise addition of the outputs of 1st and 2nd parts of the ACNN layer:\n",
    "            added_outputs = tf.add(auto_correlated_input, convolved_input)\n",
    "            auto_correlation = tf.nn.relu(added_outputs)\n",
    "            self.combine_acnn_features.append(auto_correlation)\n",
    "\n",
    "        # Combine all the ACNN features:\n",
    "        num_filters_total = num_filters * len(conv1_filter_sizes)\n",
    "        self.all_acnn_features = tf.concat(self.combine_acnn_features, 3)\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                        Convolutional Layer\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.combine_conv3_features = []\n",
    "        conv3_input = tf.reshape(self.all_acnn_features, [-1, max_length, num_filters_total, 1])\n",
    "        for k, filter_size in enumerate(conv3_filter_sizes):\n",
    "            conv3 = Conv2D(filters=num_filters, \n",
    "                           kernel_size=(conv3_filter_sizes[k], num_filters_total), \n",
    "                           strides=(1, num_filters_total), \n",
    "                           padding='same', \n",
    "                           activation='relu', \n",
    "                           name=\"conv3\")\n",
    "            conv3_output = conv3(conv3_input)\n",
    "            self.combine_conv3_features.append(conv3_output)\n",
    "\n",
    "        # Combine all the conv3 features:\n",
    "        num_filters_total = num_filters * len(conv3_filter_sizes)\n",
    "        self.all_conv3_features = tf.concat(self.combine_conv3_features, 3)\n",
    "        self.reshaped_conv3_features = tf.reshape(self.all_conv3_features, [-1, num_filters_total])\n",
    "\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        #                         1-Width Convolution\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Fully Connected Layer\n",
    "        self.l2_loss = tf.constant(0.0) # keeping track of l2 regularization loss\n",
    "        self.local4 = Dense(units=128, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            activation='relu', \n",
    "                            name=\"local4\")\n",
    "        local4_output = self.local4(self.reshaped_conv3_features)\n",
    "        self.l2_loss += tf.nn.l2_loss(self.local4.weights[0]) # weights[0] should give the kernel weights\n",
    "\n",
    "        # Final scores and predictions\n",
    "        self.output = Dense(units=num_classes, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            activation='sigmoid', \n",
    "                            name=\"output\")\n",
    "        self.scores = self.output(local4_output)\n",
    "        self.l2_loss += tf.nn.l2_loss(self.output.weights[0]) # weights[0] should give the kernel weights\n",
    "        self.probs = tf.sigmoid(self.scores, name=\"sigmoid\") \n",
    "        condition = tf.less(self.probs, tf.fill(tf.shape(self.probs), 0.5))\n",
    "        self.predictions = tf.where(\n",
    "            condition, tf.zeros(tf.shape(self.probs)), tf.ones(tf.shape(self.probs))\n",
    "            ) # if prob >= 0.5 ==> 1 (i.e. disfluent); else ==> 0 (i.e. fluent)\n",
    "\n",
    "\n",
    "        # Calculate sigmoid cross entropy loss\n",
    "        self.loss_fn = BinaryCrossentropy(from_logits=True)\n",
    "        losses = self.loss_fn(self.input_y, self.scores)\n",
    "        masked_losses = tf.transpose(self.input_z) * losses\n",
    "        self.loss = (tf.reduce_sum(masked_losses) / tf.cast(self.batch_size, \"float32\")) \\\n",
    "                    + (self.l2_reg_lambda * self.l2_loss)\n",
    "\n",
    "        # Calculate f-score\n",
    "        fscore_mask = tf.cast(self.input_z, \"int64\")\n",
    "        predictions = tf.cast(self.predictions, \"int64\")\n",
    "        input_y = tf.cast(self.input_y, \"int64\")\n",
    "\n",
    "        # e ==> num of edited predictions\n",
    "        masked_prediction = fscore_mask * predictions\n",
    "        e = tf.reduce_sum(masked_prediction)\n",
    "        self.nprediction = tf.cast(e, dtype=tf.int32, name=\"nprediction\")\n",
    "\n",
    "        # g ==> num of edited words in gold set\n",
    "        masked_input_y = fscore_mask * input_y\n",
    "        g = tf.reduce_sum(masked_input_y)\n",
    "\n",
    "## End of ACNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
